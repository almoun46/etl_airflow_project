# ğŸŒ¤ï¸ Weather ETL Pipeline avec Airflow, PostgreSQL et Streamlit

Ce projet consiste en un pipeline ETL (Extract, Transform, Load) automatisÃ© par Airflow pour rÃ©cupÃ©rer des donnÃ©es 
mÃ©tÃ©orologiques via une API, les transformer, puis les stocker dans une base de donnÃ©es PostgreSQL. Une interface Streamlit
permet de visualiser les donnÃ©es collectÃ©es sous forme graphique.

---

## ğŸš€ Objectifs du projet

- Automatiser la rÃ©cupÃ©ration quotidienne de donnÃ©es mÃ©tÃ©o.
- Transformer et nettoyer les donnÃ©es rÃ©cupÃ©rÃ©es.
- Stocker efficacement ces donnÃ©es dans PostgreSQL.
- Fournir une interface interactive pour visualiser les rÃ©sultats avec Streamlit.

---

## ğŸ› ï¸ Technologies utilisÃ©es

- **Python**
- **Airflow** (Orchestration)
- **PostgreSQL** (Base de donnÃ©es)
- **Docker Compose** (Conteneurisation)
- **Streamlit** (Dashboard interactif)

---

## ğŸ“‚ Structure du projet

```bash
project/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_pipeline.py          # DAG Airflow
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py                  # Dashboard Streamlit
â”œâ”€â”€ docker-compose.yml           # Configuration Docker
â”œâ”€â”€ requirements.txt             # 
â””â”€â”€ logs/                        # Logs Airflow
```


---

## ğŸ—ºï¸ Architecture du pipeline

Voici une vue d'ensemble du pipeline de donnÃ©es mÃ©tÃ©o automatisÃ© :

![Architecture du pipeline](./architecture.png)

- **API MÃ©tÃ©o** : Source des donnÃ©es mÃ©tÃ©o quotidiennes
- **Airflow** : Orchestration des Ã©tapes Extract â†’ Transform â†’ Load
- **PostgreSQL** : Stockage structurÃ© des donnÃ©es
- **Streamlit** : Visualisation des donnÃ©es mÃ©tÃ©o
- **Docker Compose** : Conteneurisation et dÃ©ploiement local

---




## âš™ï¸ Installation

### 1. Cloner le dÃ©pÃ´t

```bash
git clone <URL_GITHUB>
cd project
```

### 2. Lancer l'environnement avec Docker Compose

```bash
docker-compose up --build -d
```

### 3. AccÃ©der aux services

- Airflow Webserver: [http://localhost:8080](http://localhost:8080)
  - **Utilisateur:** `admin` / **Mot de passe:** `admin`

- Dashboard Streamlit: [http://localhost:8501](http://localhost:8501)

- PostgreSQL:
  - **Host:** `localhost`
  - **Port:** `5432`
  - **Utilisateur:** `airflow`
  - **Mot de passe:** `airflow`
  - **Base de donnÃ©es:** `airflow`

---

## ğŸ“ˆ Utilisation

Le DAG Airflow nommÃ© `etl_pipeline` s'exÃ©cute quotidiennement et effectue les tÃ¢ches suivantes :

- **Extraction** des donnÃ©es depuis l'API mÃ©tÃ©o (Open-Meteo).
- **Transformation** des donnÃ©es extraites en format appropriÃ© pour la base de donnÃ©es.
- **Chargement** des donnÃ©es transformÃ©es dans PostgreSQL.

Le Dashboard Streamlit affiche :
- Un tableau avec les donnÃ©es mÃ©tÃ©o (tempÃ©ratures min/max, prÃ©cipitations, etc.).
- Un graphique montrant l'Ã©volution quotidienne des tempÃ©ratures et prÃ©cipitations.

---

## ğŸ“Œ Exemples de visuels

*Capture d'Ã©cran du dashboard Streamlit
![capture du dashboard]((./visuel_temp.png) .*

---

## âš ï¸ Points Ã  amÃ©liorer

- Gestion d'erreurs complÃ¨te (API indisponible, connexion DB).
- Ajout d'un systÃ¨me de monitoring (logs avancÃ©s, alertes).
- DÃ©ploiement cloud pour simuler un environnement de production rÃ©el.

---

## ğŸ’¡ Auteur

- **Almountassir Abdel-aziz** â€“ 

---

ğŸ¯ **Next steps** : intÃ©gration d'un modÃ¨le ML pour prÃ©dire les donnÃ©es mÃ©tÃ©o futures ! ğŸš€

